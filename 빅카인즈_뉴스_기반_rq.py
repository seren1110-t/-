# -*- coding: utf-8 -*-
"""빅카인즈_뉴스_기반_RQ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sZAnkUjlXzGbSeRRk-WHQ6nNRpexhDG8
"""

from google.colab import drive

# Google Drive 마운트
drive.mount('/content/drive')

import pandas as pd
#빅카인즈에서 데이터 파일 다운로드
# 파일 경로 설정
file_path = "/content/drive/MyDrive/NewsResult_20250220-20250520.xlsx"  # 경로를 정확하게 확인하고 수정해!

# 엑셀 파일 읽기
interests_df = pd.read_excel(file_path)

interests_df = interests_df[['일자', '제목',  '본문', 'URL']]

#!pip install langchain chromadb openai tiktoken sentence-transformers
!pip install -U langchain-community
!pip install rank_bm25

!pip install faiss-cpu

import pandas as pd
import re


# 2. 텍스트 정제 함수
def clean_korean_text(text):
    if pd.isnull(text):
        return ''
    text = re.sub(r'[\n\t\r]', ' ', text)           # 줄바꿈 문자 제거
    text = re.sub(r'[^\uAC00-\uD7A3a-zA-Z0-9 .,?!]', '', text)  # 한글, 영문, 숫자 외 제거
    text = re.sub(r'\s+', ' ', text)                # 다중 공백 제거
    return text.strip()

# 3. 제목/본문 전처리 적용
interests_df['제목'] = interests_df['제목'].apply(clean_korean_text)
interests_df['본문'] = interests_df['본문'].apply(clean_korean_text)

# 4. Null 제거
interests_df.dropna(subset=['제목', '본문'], inplace=True)

# 5. 텍스트 결합 (RAG에 쓸 전체 텍스트)
interests_df['내용'] = interests_df['제목'] + '\n' + interests_df['본문']

import pandas as pd
import os
import openai
from langchain.schema import Document
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.vectorstores import FAISS

# API 키 설정 (Colab 환경)
from google.colab import userdata
os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')
openai.api_key = os.getenv("OPENAI_API_KEY")

# interests_df 존재 여부 확인 필요

# 1. 문서 변환
docs = []
for idx, row in interests_df.iterrows():
    metadata = {
        "일자": str(row["일자"]),
        "제목": row["제목"],
        "URL": row["URL"]
    }
    content = row["내용"].replace("\n", " ").strip()
    docs.append(Document(page_content=content, metadata=metadata))

## 2. BM25 Retriever 생성
#bm25_retriever = BM25Retriever.from_documents(docs)
#bm25_retriever.k = 5

# 3. 임베딩 모델 (GPU 사용 가능 확인 필요)
embedding_model = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model_kwargs={'device': 'cuda'}  # GPU 없으면 'cpu'로 변경
)

# 4. FAISS DB 생성
faiss_db = FAISS.from_documents(docs, embedding_model)

## 5. Faiss retriever 생성
#faiss_retriever = faiss_db.as_retriever(search_kwargs={"k": 5})

## 6. 앙상블 Retriever (BM25 + Faiss)
#ensemble_retriever = EnsembleRetriever(
#    retrievers=[bm25_retriever, faiss_retriever],
#    weights=[0.6, 0.4],
#    top_k=5
#)

# 7. LLM 및 QA 체인 구성
#llm = ChatOpenAI(model_name="gpt-4o", api_key=openai.api_key, temperature=0)

#qa = RetrievalQA.from_chain_type(
#    llm=llm,
#    chain_type="stuff",
#    retriever=ensemble_retriever
#)

qa.invoke("가장최근 삼성기사")

import pickle

with open("bk_docs.pkl", "wb") as f:
    pickle.dump(docs, f)

#파일명	역할 설명
#1. index.faiss	실제 벡터 인덱스 데이터 (FAISS 인덱스 객체)
#- 즉, 벡터들의 수치적 표현을 빠르게 검색할 수 있도록 만든 데이터 구조 (예: IVF, HNSW 등)
#2. index.pkl	벡터와 연결된 metadata 와 기타 부가 정보 저장
#(예: 원문 텍스트, 문서의 제목, 날짜, URL, etc)


faiss_db.save_local("bk_faiss_index")



"""2. Faiss를 Chroma보다 추천하는 이유
성능(속도)
Faiss는 Facebook AI Research에서 개발한 라이브러리로, 대규모 벡터 검색에 최적화되어 있어 매우 빠르고 효율적인 벡터 유사도 검색이 가능합니다. 특히 수백만 개 이상의 벡터를 다룰 때 속도와 메모리 효율 면에서 뛰어납니다.

유연성
Faiss는 다양한 인덱싱 기법(PQ, IVF, HNSW 등)을 지원하며, 벡터 검색을 위한 고급 튜닝이 가능해 사용자의 요구에 맞춰 최적화할 수 있습니다.

확장성
대규모 데이터셋 처리에 유리하며, GPU 가속도 지원하여 벡터 검색 속도를 더욱 높일 수 있습니다.

오픈소스 커뮤니티와 생태계
Faiss는 오랫동안 벡터 검색 분야에서 널리 사용되어 왔고, 관련 자료와 튜토리얼, 커뮤니티 지원이 풍부합니다."""

qa("삼성주가 긍정적이니?")







